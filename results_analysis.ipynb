{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import subprocess\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import h5py\n",
    "from openTSNE import TSNE\n",
    "import matplotlib.colors as mcolors\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "import matplotlib.cm as cm\n",
    "import sqlalchemy\n",
    "import pyodbc\n",
    "import urllib\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import pyarrow.parquet as pq\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_file = pq.ParquetFile('file')\n",
    "num_rows = parquet_file.metadata.num_rows\n",
    "num_subparts = 100\n",
    "batch_size = num_rows // num_subparts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunks = []\n",
    "for i in parquet_file.iter_batches(batch_size=batch_size, use_pandas_metadata=True):\n",
    "    df_chunks.append(i.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(df_chunks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_str'] = df.date.apply(lambda x: x.strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.read_parquet('total_keywords_with_embeddings.parquet')\n",
    "# temp = temp.drop('repere', axis=1)\n",
    "# df = df.drop('cluster', axis=1)\n",
    "# df = pd.concat([df, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_df = pd.read_parquet('cluster_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords_df = pd.read_parquet('all_keywords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---GLOBAL STATS---')\n",
    "print()\n",
    "print('oldest date: ' + str(df.date.min()))\n",
    "print('latest date: ' + str(df.date.max()))\n",
    "print()\n",
    "print('number of articles: ' + str(len(df)))\n",
    "print('number of unique articles: ' + str(len(list(set(df.content.values.tolist())))))\n",
    "print('number of unique tickers: ' + str(len(df.ticker.unique())))\n",
    "print()\n",
    "print('min len of articles: ' + str(df.content.str.len().min()) + ' characters')\n",
    "print('max len of articles: ' + str(df.content.str.len().max()) + ' characters')\n",
    "print('mean len of articles: ' + str(df.content.str.len().mean()) + ' characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---KEYWORDS STATS---')\n",
    "print()\n",
    "print('total number of keywords: ' + str(sum([len(row) for row in df['keywords']])))\n",
    "print('number of unique keywords: ' + str(len(list(set([x for sublist in df.keywords.values.tolist() for x in sublist])))))\n",
    "print('mean number of keywords per article: ' + str(df['keywords'].apply(lambda x: len(x)).mean()))\n",
    "print()\n",
    "print('top 10 most appearing keywords: \\n' + str(pd.DataFrame([x for sublist in df.keywords.values.tolist() for x in sublist]).value_counts().head(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrapped_tickers = []\n",
    "unwrapped_keywords = []\n",
    "unwrapped_embeddings = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    ticker = row['ticker']\n",
    "    keywords = row['keywords']\n",
    "    embeddings = row['keywords_embeddings']\n",
    "    \n",
    "    for x,_ in enumerate(keywords):\n",
    "        unwrapped_tickers.append(ticker)\n",
    "        unwrapped_keywords.append(keywords[x])\n",
    "        unwrapped_embeddings.append(embeddings[x])\n",
    "\n",
    "result = {\n",
    "    'ticker': unwrapped_tickers,\n",
    "    'keyword': unwrapped_keywords,\n",
    "    'embedding': unwrapped_embeddings\n",
    "}\n",
    "\n",
    "keywords_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df.keyword = keywords_df.keyword.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df = keywords_df.drop_duplicates(subset='keyword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(keywords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(keywords_df['embedding'].tolist())\n",
    "\n",
    "# Specify the number of clusters (k)\n",
    "k = 1000\n",
    "\n",
    "# Perform K-means clustering\n",
    "centroids, distortion = kmeans(data, k)\n",
    "\n",
    "# Assign data points to clusters\n",
    "labels, _ = vq(data, centroids)\n",
    "\n",
    "# Add the cluster labels to the DataFrame\n",
    "keywords_df['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords_df.to_parquet('all_keywords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = df['keywords'].apply(lambda x: [keywords_df.loc[keywords_df['keyword'] == y, 'cluster'].values[0] for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_parquet('full_merged_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = df.explode('cluster')[['date', 'cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df['value'] = 1\n",
    "cluster_df.date = cluster_df.date.dt.strftime('%Y-%m-%d')\n",
    "cluster_df.date = pd.to_datetime(cluster_df.date)\n",
    "cluster_df = cluster_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df['value'] = cluster_df.groupby([cluster_df.index, 'cluster'])['value'].transform('sum')\n",
    "cluster_df = cluster_df.reset_index()\n",
    "cluster_df = cluster_df.groupby('cluster').apply(lambda x: x.drop_duplicates(subset='date'))\n",
    "cluster_df = cluster_df.reset_index(drop=True)\n",
    "cluster_df = cluster_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.value = cluster_df.groupby('cluster').value.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "cluster_df.groupby('cluster').value.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_df.groupby('cluster').filter(lambda x: len(x) >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_df.groupby('cluster').apply(lambda x: x.resample('D').ffill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_df.droplevel('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_slope(df):\n",
    "    df = df.reset_index()\n",
    "    y = df['value'].values\n",
    "    df['slope'] = np.gradient(y, df.index)\n",
    "    df = df.set_index('date')\n",
    "    return df\n",
    "\n",
    "cluster_df = cluster_df.groupby('cluster').apply(calculate_slope)\n",
    "\n",
    "def calculate_slope_gap(df):\n",
    "    df['slope_gap'] = np.log(df.slope) - np.log(df.shift(1).slope)\n",
    "    return df\n",
    "\n",
    "cluster_df = cluster_df.groupby('cluster').apply(calculate_slope_gap)\n",
    "\n",
    "def calculate_slope_diff(df):\n",
    "    df['slope_diff'] = df.slope.diff()\n",
    "    return df\n",
    "\n",
    "cluster_df = cluster_df.groupby('cluster').apply(calculate_slope_diff)\n",
    "\n",
    "def calculate_rolling_slope(df):\n",
    "    df['rolling_slope'] = df.slope.rolling(15).mean()\n",
    "    return df\n",
    "\n",
    "# Assuming you have already created and populated the 'cluster' column in x_df\n",
    "cluster_df = cluster_df.groupby('cluster').apply(calculate_rolling_slope)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "cluster_df.groupby('cluster').slope.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_df.to_parquet('cluster_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = df.ticker.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serv_name_smartbeta = \\\n",
    "    \"\"\"DRIVER={SQL Server};SERVER=sqlsmartbetaprod\\\\smartbetaprod;\n",
    "        DATABASE=SMARTBETA_PROD;Trusted_Connection='Yes''\"\"\"\n",
    "smartbeta = pyodbc.connect(serv_name_smartbeta)\n",
    "\n",
    "# smartbeta server sqlalchemy connection\n",
    "quote_smartbeta = \\\n",
    "    urllib.parse.quote_plus(serv_name_smartbeta)\n",
    "sqlalch_conn = \\\n",
    "    r'mssql+pyodbc:///?odbc_connect={}'\\\n",
    "    .format(quote_smartbeta)\n",
    "engine = sqlalchemy.create_engine(sqlalch_conn)\n",
    "conn = smartbeta.cursor()\n",
    "\n",
    "ticker_for_req = [' '.join([x.split('@')[0] ,x.split('@')[1].replace('GR', 'GY').replace('SW', 'SE').replace('SM', 'SQ')]) for x in tickers]\n",
    "\n",
    "ticker_str = '(' + ', '.join([f\"'{ticker}'\" for ticker in ticker_for_req]) + ')'\n",
    "\n",
    "start_date = '2018-01-01'\n",
    "query = f\"\"\"select TT.fsym_id,\n",
    "       TT.date,\n",
    "       TT.tot_ret_euro,\n",
    "       TT.Bloom_Nego   \n",
    "from\n",
    "(\n",
    "SELECT  *\n",
    "FROM [SMARTBETA_PROD].[dbo].[data_Tot_Ret_daily] as TR\n",
    "join\n",
    "equity_info_codes() EIC\n",
    "on EIC.fsym_regional_id = TR.fsym_id\n",
    "where TR.date >= '2018-01-01' \n",
    "and EIC.Bloom_Nego in {ticker_str}) as TT\"\"\"\n",
    "\n",
    "query_cac = f\"\"\"SELECT II.Code_instrument,\n",
    "        II.Code_Bloom,\n",
    "                                   RTRIM(II.Devise) as Devise,\n",
    "                                   HI.Cours_Local,\n",
    "                                   HI.date\n",
    "                            FROM [SMARTBETA_PROD].[dbo].[instr_Index] II\n",
    "                            JOIN [SMARTBETA_PROD].[dbo].[histo_instruments] HI\n",
    "                            ON II.Code_instrument = HI.Code_instrument\n",
    "                            WHERE II.Code_Bloom = 'SXXP Index'\n",
    "                            AND HI.date >= '2018-01-01'\"\"\"\n",
    "\n",
    "sxxp_returns = pd.read_sql_query(query_cac, engine)\n",
    "daily_returns = pd.read_sql_query(query, engine)\n",
    "daily_returns.date = pd.to_datetime(daily_returns.date)\n",
    "\n",
    "pivot_df = daily_returns.pivot_table(index='date', columns='Bloom_Nego', values='tot_ret_euro')\n",
    "pivot_df.columns = ['@'.join(x.split(' ')) for x in pivot_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.iloc[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sxxp_returns.date = pd.to_datetime(sxxp_returns.date)\n",
    "sxxp_returns = sxxp_returns.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_tickers = [x for x in ticker_for_req if '@'.join(x.split(' ')) not in pivot_df.columns]\n",
    "print(len(missing_tickers))\n",
    "missing_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ticker = df.ticker.apply(lambda x: '@'.join([x.split('@')[0] ,x.split('@')[1].replace('GR', 'GY').replace('SW', 'SE').replace('SM', 'SQ')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df[cluster_df.slope >=12].cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_keywords(cluster):\n",
    "    print(keywords_df[keywords_df.cluster == cluster].keyword.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trending_clusters_overview(cluster):\n",
    "    selected_cluster_df = df[df.cluster.apply(lambda x: cluster in x)]\n",
    "    filtered_df = cluster_df[cluster_df.cluster == cluster]\n",
    "    selected_cluster_df = selected_cluster_df[selected_cluster_df.date > '2017-01-01']\n",
    "    selected_cluster_df['overall'] = 1\n",
    "\n",
    "\n",
    "    monthly_article_counts = selected_cluster_df.pivot_table(index='year_month', columns='ticker', values='content', aggfunc='count', fill_value=0)\n",
    "\n",
    "    weekly_article_counts = selected_cluster_df.pivot_table(index='year_week', columns='ticker', values='content', aggfunc='count', fill_value=0)\n",
    "\n",
    "    daily_article_counts = selected_cluster_df.pivot_table(index='date', columns='ticker', values='content', aggfunc='count', fill_value=0)\n",
    "\n",
    "\n",
    "    monthly_article_counts[[ticker for ticker in pivot_df.columns if ticker not in monthly_article_counts]] = 0\n",
    "    weekly_article_counts[[ticker for ticker in pivot_df.columns if ticker not in weekly_article_counts]] = 0\n",
    "    daily_article_counts[[ticker for ticker in pivot_df.columns if ticker not in daily_article_counts]] = 0\n",
    "\n",
    "\n",
    "    monthly_article_counts = monthly_article_counts.shift(1).fillna(0.0)\n",
    "    weekly_article_counts = weekly_article_counts.shift(1).fillna(0.0)\n",
    "    daily_article_counts = daily_article_counts.shift(1).fillna(0.0)\n",
    "\n",
    "\n",
    "    monthly_article_counts = monthly_article_counts[[x for x in monthly_article_counts.columns if x in pivot_df.columns]]\n",
    "\n",
    "    weekly_article_counts = weekly_article_counts[[x for x in weekly_article_counts.columns if x in pivot_df.columns]]\n",
    "\n",
    "    daily_article_counts = daily_article_counts[[x for x in daily_article_counts.columns if x in pivot_df.columns]]\n",
    "\n",
    "\n",
    "    monthly_weights = monthly_article_counts.div(monthly_article_counts.sum(axis=1), axis=0)\n",
    "\n",
    "    weekly_weights = weekly_article_counts.div(weekly_article_counts.sum(axis=1), axis=0)\n",
    "\n",
    "    daily_weights = daily_article_counts.div(daily_article_counts.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "    cluster_tickers_returns = pivot_df[[x for x in monthly_weights.columns]]\n",
    "\n",
    "\n",
    "    monthly_weights.index = pd.to_datetime(monthly_weights.index)\n",
    "\n",
    "    weekly_weights.index = pd.to_datetime(weekly_weights.index + '-0', format='%Y-%U-%w')\n",
    "\n",
    "    daily_weights = daily_weights.resample('D').sum()\n",
    "    daily_weights.index = pd.to_datetime(daily_weights.index.strftime('%Y-%m-%d'))\n",
    "\n",
    "    cluster_tickers_returns.index = pd.to_datetime(cluster_tickers_returns.index)\n",
    "\n",
    "\n",
    "    weekly_weights = weekly_weights[~weekly_weights.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "    monthly_weights = monthly_weights.reindex(cluster_tickers_returns.index, method='ffill')\n",
    "\n",
    "    weekly_weights = weekly_weights.reindex(cluster_tickers_returns.index, method='ffill')\n",
    "\n",
    "    daily_weights = daily_weights.reindex(cluster_tickers_returns.index, method='ffill')\n",
    "\n",
    "\n",
    "    monthly_rebalanced_weighted_returns = cluster_tickers_returns * monthly_weights\n",
    "\n",
    "    weekly_rebalanced_weighted_returns = cluster_tickers_returns * weekly_weights\n",
    "\n",
    "    daily_rebalanced_weighted_returns = cluster_tickers_returns * daily_weights\n",
    "\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    filtered_df.slope.plot(ax=ax1, color='lightgray')\n",
    "    filtered_df.rolling_slope.plot(ax=ax1, color='lightblue')\n",
    "    ax1.legend(loc='upper left')\n",
    "\n",
    "    # ax2 = ax1.twinx()\n",
    "    # filtered_df.value.plot(ax=ax2, color='r', label='cluster cumulative appearance')\n",
    "    # ax2.legend(loc='lower right')\n",
    "\n",
    "    ax3 = ax1.twinx()\n",
    "    (monthly_rebalanced_weighted_returns.sum(axis=1).cumsum()/sxxp_returns.Cours_Local).plot(ax=ax3, figsize=(18, 12), label='custom monthly rebalanced portfolio returns / index returns', color='orange')\n",
    "    (weekly_rebalanced_weighted_returns.sum(axis=1).cumsum()/sxxp_returns.Cours_Local).plot(ax=ax3, label='custom weekly rebalanced portfolio returns / index returns', color='purple')\n",
    "    (daily_rebalanced_weighted_returns.sum(axis=1).cumsum()/sxxp_returns.Cours_Local).plot(ax=ax3, label='custom daily rebalanced portfolio returns / index returns', color='red')\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    (monthly_weights.max(axis=1)/monthly_weights.sum(axis=1)).plot(figsize=(15,9))\n",
    "    (weekly_weights.max(axis=1)/weekly_weights.sum(axis=1)).plot(figsize=(15,9))\n",
    "    (daily_weights.max(axis=1)/daily_weights.sum(axis=1)).plot(figsize=(15,9))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_df[keywords_df.keyword.str.contains('vaccine')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_dropdown = widgets.Dropdown(options=cluster_df[cluster_df.slope >=12].cluster.unique().tolist())\n",
    "\n",
    "# Use the interact function with the Dropdown widget\n",
    "interact(trending_clusters_overview, cluster=int_dropdown)\n",
    "\n",
    "interact(display_cluster_keywords, cluster=int_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_cluster_content(cluster, start_date, snapshot_date):\n",
    "\n",
    "    start_date = start_date.strftime('%Y-%m-%d')\n",
    "    snapshot_date = snapshot_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    selected_cluster_df = df[df.cluster.apply(lambda x: cluster in x)]\n",
    "    selected_cluster_df = selected_cluster_df[selected_cluster_df.date >= start_date]\n",
    "    selected_cluster_df['overall'] = 1\n",
    "\n",
    "\n",
    "    monthly_article_counts = selected_cluster_df.pivot_table(index='year_month', columns='ticker', values='content', aggfunc='count', fill_value=0)\n",
    "\n",
    "    weekly_article_counts = selected_cluster_df.pivot_table(index='year_week', columns='ticker', values='content', aggfunc='count', fill_value=0)\n",
    "\n",
    "    daily_article_counts = selected_cluster_df.pivot_table(index='date', columns='ticker', values='content', aggfunc='count', fill_value=0)\n",
    "\n",
    "\n",
    "    monthly_article_counts[[ticker for ticker in pivot_df.columns if ticker not in monthly_article_counts]] = 0\n",
    "    weekly_article_counts[[ticker for ticker in pivot_df.columns if ticker not in weekly_article_counts]] = 0\n",
    "    daily_article_counts[[ticker for ticker in pivot_df.columns if ticker not in daily_article_counts]] = 0\n",
    "\n",
    "\n",
    "    monthly_article_counts = monthly_article_counts.shift(1).fillna(0.0)\n",
    "    weekly_article_counts = weekly_article_counts.shift(1).fillna(0.0)\n",
    "    daily_article_counts = daily_article_counts.shift(1).fillna(0.0)\n",
    "\n",
    "\n",
    "    monthly_article_counts = monthly_article_counts[[x for x in monthly_article_counts.columns if x in pivot_df.columns]]\n",
    "\n",
    "    weekly_article_counts = weekly_article_counts[[x for x in weekly_article_counts.columns if x in pivot_df.columns]]\n",
    "\n",
    "    daily_article_counts = daily_article_counts[[x for x in daily_article_counts.columns if x in pivot_df.columns]]\n",
    "\n",
    "\n",
    "    monthly_weights = monthly_article_counts.div(monthly_article_counts.sum(axis=1), axis=0)\n",
    "\n",
    "    weekly_weights = weekly_article_counts.div(weekly_article_counts.sum(axis=1), axis=0)\n",
    "\n",
    "    daily_weights = daily_article_counts.div(daily_article_counts.sum(axis=1), axis=0)\n",
    "\n",
    "\n",
    "    cluster_tickers_returns = pivot_df[[x for x in monthly_weights.columns]]\n",
    "\n",
    "\n",
    "    monthly_weights.index = pd.to_datetime(monthly_weights.index)\n",
    "\n",
    "    weekly_weights.index = pd.to_datetime(weekly_weights.index + '-0', format='%Y-%U-%w')\n",
    "\n",
    "    daily_weights = daily_weights.resample('D').sum()\n",
    "    daily_weights.index = pd.to_datetime(daily_weights.index.strftime('%Y-%m-%d'))\n",
    "\n",
    "    cluster_tickers_returns.index = pd.to_datetime(cluster_tickers_returns.index)\n",
    "\n",
    "\n",
    "    weekly_weights = weekly_weights[~weekly_weights.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "    monthly_weights = monthly_weights.reindex(cluster_tickers_returns.index, method='ffill')\n",
    "\n",
    "    weekly_weights = weekly_weights.reindex(cluster_tickers_returns.index, method='ffill')\n",
    "\n",
    "    daily_weights = daily_weights.reindex(cluster_tickers_returns.index, method='ffill')\n",
    "\n",
    "    print(f'start_date: {start_date} | snapshot_date: {snapshot_date}')\n",
    "    if snapshot_date in [x.strftime('%Y-%m-%d') for x in daily_weights.index.tolist()] and daily_weights.loc[snapshot_date].nlargest(10).max()>0:\n",
    "        print('composition:')\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[2].set_title('Daily')\n",
    "        weights = daily_weights.loc[snapshot_date].nlargest(10)\n",
    "        axs[2].pie(weights, labels=weights.index, autopct=lambda x: (x/abs(daily_weights.loc[snapshot_date]).sum()).round(2))\n",
    "        axs[2].axis('equal')\n",
    "        axs[2].set_ylabel('')\n",
    "\n",
    "        axs[1].set_title('Weekly')\n",
    "        weights = weekly_weights.loc[snapshot_date].nlargest(10)\n",
    "        axs[1].pie(weights, labels=weights.index, autopct=lambda x: (x/abs(weekly_weights.loc[snapshot_date]).sum()).round(2))\n",
    "        axs[1].axis('equal')\n",
    "        axs[1].set_ylabel('')\n",
    "\n",
    "        axs[0].set_title('Monthly')\n",
    "        weights = monthly_weights.loc[snapshot_date].nlargest(10)\n",
    "        axs[0].pie(weights, labels=weights.index, autopct=lambda x: (x/abs(monthly_weights.loc[snapshot_date]).sum()).round(2))\n",
    "        axs[0].axis('equal')\n",
    "        axs[0].set_ylabel('')\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('weights are NaNs because of shifting/rolliing OR date not found in daily returns index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dates = df.day_str.unique()\n",
    "dt_valid = [datetime.datetime.strptime(x, '%Y-%m-%d').date() for x in valid_dates]\n",
    "\n",
    "start_date = widgets.DatePicker(\n",
    "    description='Start Date',\n",
    "    value=(min(dt_valid)),\n",
    "    min=(min(dt_valid)),\n",
    "    max=(max(dt_valid))\n",
    "    )\n",
    "snapshot_date = widgets.DatePicker(\n",
    "    description='End Date',\n",
    "    value=(min(dt_valid)),\n",
    "    min=(min(dt_valid)),\n",
    "    max=(max(dt_valid))\n",
    "    )\n",
    "\n",
    "interact(analyse_cluster_content, cluster=widgets.IntSlider(min=0, max=1000), start_date=start_date, snapshot_date=snapshot_date)\n",
    "\n",
    "# analyse_cluster_content(723, '2018-01-01', '2020-12-11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LBPAM_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
